\documentclass[12pt]{article}
\usepackage{float}
%\usepackage[ruled,vlined,linesnumbered,algo2e]{algorithm2e}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}

\usepackage{pgfplots}
% and optionally (as of Pgfplots 1.3):
\pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}
\newlength\figureheight
\newlength\figurewidth


\usepackage[a4paper, total={6.5in, 9in}]{geometry}
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
   \node[shape=circle,draw=red,inner sep=1pt] (char) {#1};}}
\setlength\parindent{0pt} %% Do not touch this
\DeclareMathOperator{\phiAb}{\phi_{A,\mathbf{b}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
%% -----------------------------
%% TITLE
%% -----------------------------
\title{Short review of different beamforming techniques for passive array processing} %% Assignment Title
\author{Wissal Ghamour, Julien Gleyze and Nathan Rousselot}
%% Change "\today" by another date manually
%% -----------------------------
%% -----------------------------

%% %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%\setlength{\droptitle}{-5em}    
%% %%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle
% --------------------------
% Start here
% --------------------------
\section{Introduction}
In this document, we will cover a range of techniques specific to array processing. Array processing is a branch of signal processing dedicated to the processing of signals produced or received by an array of elements. Those element can be antennas in passive setups, or transducers in the context of radar or sonar processing. In the following, we will focus on the passive setup, meaning we work with an array of sensors. This class of problem has a wide range of applications, from radio astronomy to wireless communications. All those applications share common challenges, such as the estimation of the direction of arrival (DOA) of a signal, or optimizing the Signal-to-Interference-plus-Noise ratio (SINR). In this introductory document, we will focus ourselves on the impact of the choice of beamforming techniques, ranging from Conventional Beamforming to more robust and adaptative methods.
\section{Direct Methods}
Consider the following signal
\begin{equation}\label{eq:signal}
   \mathbf{y}(k) = \mathbf{a}_ss(k) + \mathbf{y}_{I}(k) + \mathbf{n}(k)
\end{equation}
where $\mathbf{a}_s$ is the steering vector of the signal of interest, $s(k)$ is the signal of interest, $\mathbf{y}_{I}(k)$ is the interference signal, and $\mathbf{n}(k)$ is the noise. In this section, it is assumed that $\mathbf{a}_s$ is perfectly known, meaning $\mathbf{a}_0 = \mathbf{a}_s$. The goal of beamforming is to find a weight vector $\mathbf{w}$ such that the output of the beamformer $\hat{s}(k)$ is a good estimate of $s(k)$. The output of the beamformer is given by
\begin{equation}\label{eq:bf}
   \hat{s}(k) = \mathbf{w}^H\mathbf{y}(k)
\end{equation}
\subsection{Conventional Beamformer}
The conventional beamformer is the simplest beamformer. It is defined as
\begin{equation}\label{eq:cbf}
   \mathbf{w}_{CBF} \propto \mathbf{a}(\theta_s)
\end{equation}
\begin{theorem}
    Let $\mathbf{y}(k)$ be defined by equation \ref{eq:signal} and $\mathbf{w}$ the weight vector. Considering $\mathbf{a}^H(\theta_s)\mathbf{a}(\theta_s)$ is non-singular, the coefficients of $\mathbf{w}$ that maximize the gain at $\theta_s$, subject to equation \ref{eq:cbf} is given by
    \begin{equation}
        \mathbf{w}_{CBF} = \left(\mathbf{a}^H(\theta_s)\mathbf{a}(\theta_s)\right)^{-1}\mathbf{a}(\theta_s)
    \end{equation}
\end{theorem}
\begin{proof}

\end{proof}
This beamformer is optimal when there is no-noise, and no-interference. In the vast majority of applications, this is obviously not the case. 
\subsection{Optimal Adaptative Beamformer}\label{sec:opt}
In actual conditions, CBF will prove itself as very sub-optimal. In this section we will introduce a new beamformer, that is deemed and called ``optimal''. It accounts for interferences and noise, thus fitting the model depicted in equation \ref{eq:signal}. Let us formulate this as an optimization problem. Let $\mathbf{w}\in\mathcal{W}$, the weight vector, then
\begin{equation}
    \mathbf{w}_{opt} = \max_{w\in\mathcal{W}}  \text{ SINR}(\mathbf{w})
\end{equation}
\begin{theorem}\label{thm:wopt}
    Let $\mathbf{y}(k)$ be defined following equation \ref{eq:signal} of length N. Assuming that $\forall k, \lambda_k(\mathbf{y}_I(k)+\mathbf{n}(k)) \neq 0$, with $\lambda_k(\mathbf{A})$ eigenvalues of $\mathbf{A}$, then 
    \begin{equation}\label{eq:wopt}
        \mathbf{w}_{opt} = \frac{\mathbf{C}^{-1}\mathbf{a}_s}{\mathbf{a}_s^H\mathbf{C}^{-1}\mathbf{a}_s}
    \end{equation}
\end{theorem}
\begin{proof}
From equations \ref{eq:signal} and \ref{eq:bf}, one can rewrite the output of the beamformer as
\begin{equation*}\label{eq:new_signal}
   \hat{s}(k) = \mathbf{w}^H\mathbf{a}_ss(k) + \mathbf{w}^H\mathbf{y}_{I}(k) + \mathbf{w}^H\mathbf{n}_k
\end{equation*}
\begin{equation*}
    \Rightarrow \text{ SINR}(\mathbf{w}) = \frac{\mathbb{E}\left[|\mathbf{w}^H\mathbf{a}_ss(k)|^2\right]}{\mathbb{E}\left[|\mathbf{w}^H\mathbf{y}_{I}(k) + \mathbf{w}^H\mathbf{n}(k)|^2\right]} = \frac{P_s|\mathbf{w}^H\mathbf{a}_s|^2}{\mathbf{w}^H\mathbf{C}\mathbf{w}}
\end{equation*}
Where $\mathbf{C}$ is the noise plus interference covariance matrix.
It is assumed that $\forall k, \lambda_k(\mathbf{C})\neq 0$. Thus, equation \ref{eq:wopt} rewrites
\begin{equation*}
    \mathbf{w}_{opt} = \max_{w\in\mathcal{W}} \frac{P_s|\mathbf{w}^H\mathbf{a}_s|^2}{\mathbf{w}^H\mathbf{C}\mathbf{w}}
\end{equation*}
For practical reason, let us constrain the gain to be unit
$$\begin{aligned}
    \mathbf{w}_{opt} =& \max_{w\in\mathcal{W}} &\frac{P_s|\mathbf{w}^H\mathbf{a}_s|^2}{\mathbf{w}^H\mathbf{C}\mathbf{w}}\\
    & \textrm{s.t.} \quad &|\mathbf{w}^H\mathbf{a}_s| = 1
\end{aligned} \Longleftrightarrow \begin{aligned}
    \mathbf{w}_{opt} =& \max_{w\in\mathcal{W}} &\frac{P_s}{\mathbf{w}^H\mathbf{C}\mathbf{w}}\\
    & \textrm{s.t.} \quad &|\mathbf{w}^H\mathbf{a}_s| = 1
\end{aligned}$$
Which yiels the following optimization problem
$$\begin{aligned}
    \mathbf{w}_{opt} =& \min_{w\in\mathcal{W}} &\mathbf{w}^H\mathbf{C}\mathbf{w}\\
    & \textrm{s.t.} \quad &|\mathbf{w}^H\mathbf{a}_s| = 1
\end{aligned}$$
\end{proof}
% %%%%%%%%%%%%%%%%%%%
\section{Practical Adaptative Beamforming Techniques}
In reality, the optimal beamformer (section \ref{sec:opt}) is very unpractical. Indeed, in actual conditions, the steering vector $\mathbf{a}_s$ and the covariance matrix $\mathbf{C}$ are unknown. Instead, we have approximates, respectively $\mathbf{a}_0$ and $\hat{\mathbf{C}}$.
\subsection{Minimum Variance Distortionless Response}
The Minimum Variance Distortionless Responsor (MVDR) is the direct consequence of the optimal beamformer (section \ref{sec:opt}) with approximated entries. It thus writes automatically
\begin{equation}\label{eq:mvdr}
    \mathbf{w}_{mvdr}^{smi} = \frac{\hat{\mathbf{C}}^{-1}\mathbf{a}_0}{\mathbf{a}_0^H\hat{\mathbf{C}}^{-1}\mathbf{a}_0}
\end{equation}
where $smi$ stands for ``sample matrix inversion'' and is directly linked to how $\hat{\mathbf{C}}$ is being approximated (equation \ref{eq:chat}).
\begin{equation}\label{eq:chat}
    \hat{\mathbf{C}} = \frac{1}{K}\sum_{k=1}^K \mathbf{y}(k)\mathbf{y}^H(k)
\end{equation}
where $\mathbf{y}(k) = \mathbf{y}_I(k)+\mathbf{n}(k)$
\subsection{Minimum Power Distortionless Response}
\subsubsection{Naive Approach}
Sometimes, it is not possible to isolate the noise and the interference to estimate $\mathbf{C}$. Leading us to the Minimum Power Distortionless Response beamformer (MPDR).
\begin{equation}\label{eq:mpdr}
    \mathbf{w}_{mpdr}^{smi} = \frac{\hat{\mathbf{R}}^{-1}\mathbf{a}_0}{\mathbf{a}_0^H\hat{\mathbf{R}}^{-1}\mathbf{a}_0}
\end{equation}

\begin{equation}\label{eq:mpdr_chat}
    \hat{\mathbf{R}} = \frac{1}{K}\sum_{k=1}^K \mathbf{y}(k)\mathbf{y}^H(k)
\end{equation}
where $\mathbf{y}(k) = \mathbf{a}_ss(k) + \mathbf{y}_I(k)+\mathbf{n}(k)$. Intuitively, one can think that adding the signal information to the covariance matrix will lead to signal degradation as we want to minimize its output power. 
\begin{lemma}\label{lem:mpdr}
    In case where $K\rightarrow\infty$ and $\mathbf{a}_0 = \mathbf{a}_s$, then the beam patterns of MVDR and MPDR are identical.
\end{lemma}
\begin{proof}
    Recall from equation \ref{eq:bf}
    \begin{equation*}
        \hat{s}(k)=\mathbf{w}^H\mathbf{a}_ss(k) + \mathbf{w}^H\mathbf{y}_I(k) + \mathbf{w}^H\mathbf{n}(k)
    \end{equation*}
    Now recall from Theorem \ref{thm:wopt} 
    $$\begin{aligned}
    \mathbf{w} =& \min_{w\in\mathcal{W}} &\mathbf{w}^H\mathbf{C}\mathbf{w}\\
    & \textrm{s.t.} \quad &|\mathbf{w}^H\mathbf{a}_s| = 1
\end{aligned}$$
    The constraint $|\mathbf{w}^H\mathbf{a}_s| = 1$ then ensures that the part of $\hat{s(k)}$ with the signal of interest will be preserved.
\end{proof}
In lemma \ref{lem:mpdr}, we demonstrated that MPDR and MVDR are equivalent mathematically, even while having different formulations. However, it is based uppon two strong assumptions, that are, in practice, very unrealistic. We will analyze the impact of releasing the constraint on $K$ and on $\mathbf{a_0}$ seperately.

Figure \ref{fig:mpdr_robust} illustrate the lack of robustness in MPDR method, with respect to error in the direction of arrival estimation $\Delta\theta$. It shows from figure \ref{fig:mpdr_robus_sinr} that the resulting SINR with MPDR is greatly suboptimal as soon as $\Delta\theta$ increases a bit. Looking at the array white noise gain $A_{WN}$ (figure \ref{fig:mpdr_robus_awn}, we observe that this gain dramatically decreases as $\Delta\theta$ increases. Recall that 
\begin{equation}
    A_{WN} = \|w\|^{-2} \leq N
\end{equation}
Ideally, we would want $A_{WN}=N$, and thus the pattern drawn in figure \ref{fig:mpdr_robus_awn} is worrying.
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{.4\linewidth}
        \input{paper/figures/mpdr_sinr_not_robust}
        \caption{SINR vs error in $\mathbf{a}_0$}
        \label{fig:mpdr_robus_sinr}
    \end{subfigure}\hspace{0.09\linewidth}
    \begin{subfigure}[b]{.4\linewidth}
        \input{paper/figures/mpdr_awn_not_robust}
        \caption{$A_{WN}$ vs error in $\mathbf{a}_0$}
        \label{fig:mpdr_robus_awn}
    \end{subfigure}
    \caption{Measure of robustness of MVDR and MPDR in function of the error in the direction of arrival estimation.}
    \label{fig:mpdr_robust}
\end{figure}
In figure \ref{fig:mpdr_robust} we assumed that $\mathbf{C}$ and $\mathbf{R}$ were exactly known, \textit{i.e} $K\rightarrow\infty$. As $K$ is the number of snapshots used to estimate $\mathbf{C}$ and $\mathbf{R}$, one can easily guess why it is a bold assumption. Let us assume that $\mathbf{a}_0$ is perfectly known, and then let us study the convergence properties of MVDR and MPDR beamformers in function of K.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{.4\linewidth}
        \input{paper/figures/robust_k_sinr}
        \caption{SINR in function of K}
        \label{fig:robust_k_sinr}
    \end{subfigure}\hspace{0.09\linewidth}
    \begin{subfigure}[b]{.4\linewidth}
        \input{paper/figures/robust_k_awn}
        \caption{$A_{WN}$ in function of K}
        \label{fig:robust_k_awn}
    \end{subfigure}
    \caption{Convergence in function of K of MVDR and MPDR.}
    \label{fig:mpdr_robust}
\end{figure}

It appears from both figures \ref{fig:robust_k_sinr} and \ref{fig:robust_k_awn} that MPDR requires way more snapshots than MVDR to converge. To understand where this lack of performance of MPDR comes from, figure \ref{fig:poor_mvdr_beampattern} gives a hint. Recall from equation \ref{eq:mpdr_chat} that the covariance matrix in the MPDR beamformer contains the signal of interest. Now, from equation \ref{eq:mpdr}, we recall that the optimization problem with respect to this covariance matrix tends to add zeros to the respective components (usually, the interferences) of the covariance matrix. When there is a significant error in the distance of arrival estimation, as illustrated in figure \ref{fig:poor_mvdr_beampattern}, then, the MPDR beamformer will naturally consider the signal of interest as interference, and put a zero where it is coming from. This explains why MPDR, as seen so far, is lacking a lot of robustness, and cannot be used in practice.

\begin{figure}[H]
    \centering
    \input{paper/figures/poor_mvdr_beampattern}
    \caption{Beampatterns comparison between MVDR, MPDR, CBF and Optimal bemformers. We can observe the zero that has been set right in the direction of arrival of MPDR, thus creating the loss in SINR/$A_{WN}$}
    \label{fig:poor_mvdr_beampattern}
\end{figure}
\subsubsection{Robust MPDR}
From the previous section, it seems that MPDR is hardly usable in practice. However, while MVDR is seemingly a better option, it requires being able to measure the noise plus interference covariance matrix, without the signal of interest. This can be the case, for example, in radio-astronomy. We need to find tricks that enables the use of MPDR. In this section, we demonstrate that by constraining the $A_{WN}$ gain to be above a certain threshold, close to $N$, we can robustify MPDR. This means we want to solve 
$$\begin{aligned}
    \mathbf{w}_{mpdr} =& \min_{w\in\mathcal{W}} &\mathbf{w}^H\mathbf{C}\mathbf{w}\\
    & \textrm{s.t.} \quad &|\mathbf{w}^H\mathbf{a}_s| = 1\\
    & \textrm{s.t.} \quad &\|w\|^{-2} \geq \alpha N
\end{aligned}$$
where $\alpha$ is a constant. This optimization problem is a Quadratically Constrained Quadratic Program (QCQP). It can be solved using the Lagrangian method. The Lagrangian of this problem is given by
\begin{equation}
    \mathcal{L}(\mathbf{w},\lambda,\mu) = \mathbf{w}^H\mathbf{C}\mathbf{w} + \lambda\left(|\mathbf{w}^H\mathbf{a}_s| - 1\right) + \mu\left(\|w\|^{-2} - \alpha N\right)
\end{equation}
where $\lambda$ and $\mu$ are the Lagrange multipliers. After some simplifications, and by enforcing $\mathbf{w}^H\mathbf{a}_0 = 1$, we get
\begin{equation}\label{eq:mpdr_dl}
    \mathbf{w}_{MPDR-DL} = \frac{\left(\hat{\mathbf{R}}+\mu \mathbf{I}\right)^{-1}\mathbf{a}_0}{\mathbf{a}_0^H\left(\hat{\mathbf{R}}+\mu \mathbf{I}\right)^{-1}\mathbf{a}_0}
\end{equation}
We can note that when $\mu = 0$, we get the naive MPDR beamformer. When $\mu\rightarrow\infty$ we get the CBF beamformer. We thus showed that this new constraied method is, in fact, a matter of compromise between MPDR and CBF. Figure \ref{fig:MPDR_robust} shows the robustness of this new method, with respect to the error in the direction of arrival estimation. We can observe that the SINR is greatly improved, and that the $A_{WN}$ is close to $N$.
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{.4\linewidth}
        \input{paper/figures/MPDR_robust_sinr}
        \caption{SINR in function of $\Delta\theta$}
        \label{fig:MPDR_robust_sinr}
    \end{subfigure}\hspace{0.09\linewidth}
    \begin{subfigure}[b]{.4\linewidth}
        \input{paper/figures/MPDR_robust_awn}
        \caption{$A_{WN}$ in function of $\Delta\theta$}
        \label{fig:MPDR_robust_awn}
    \end{subfigure}
    \caption{Robustness of the new MPDR method in function of the error in the direction of arrival estimation.}
    \label{fig:MPDR_robust}
\end{figure}
Looking at the rate of convergence of this new method, we can observe that it is similar to the one of MVDR, as shown in figure \ref{fig:robust_k_sinr} and \ref{fig:robust_k_awn}. This is a good news, as it means that we can use this new method in practice, and that it is not too sensitive to the number of snapshots used to estimate $\mathbf{R}$.
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\linewidth}
        \input{paper/figures/MPDR_robust_awn_k}
        \caption{SINR in function of K}
        \label{fig:robust_k_sinr}
    \end{subfigure}\hspace{0.09\linewidth}
    \begin{subfigure}[b]{0.4\linewidth}
        \input{paper/figures/MPDR_robust_awn_k}
        \caption{$A_{WN}$ in function of K}
        \label{fig:robust_k_awn}
    \end{subfigure}
    \caption{Convergence in function of K of MVDR and MPDR.}
    \label{fig:mpdr_robust_k}
\end{figure}
An issue with that method is that it requires to know the optimal value of $\mu$, which can be hard in practice as its closed form solution is very hard (close to impossible) to determine. It needs to be done iteratively, and its form is depicted in figure \ref{fig:mu}. 
\begin{figure}[H]
    \centering
    \input{figures/mu_opt}
    \caption{Value of $SINR$ in function of $\mu$}
    \label{fig:mu}
\end{figure}
Figure \ref{fig:mu} shows that, while it is iteratively hard to determine the optimal $\mu$, it seems like this optimum value is not very sensitive, and a gross guess around it should provide with very good (enough) results. 
\subsubsection{Unit-Circle MPDR}
In this section, we will study another approach in making MPDR a robust beamformer. This time, we will take a radically different approach, which does not have any hyperparameter to tune. First, let us consider a vector $\mathbf{w}$. Its associated polynomial is given by
\begin{equation}
    P(z)=\sum_{n=1}^N \mathbf{w}(n)z^{-n}
\end{equation}
An interesting fact, is that for $\mathbf{w}_{opt}$, all the zeroes of $P_{opt}(z)$ are on the unit-circle. In figure \ref{fig:uc_monte_carlo}, we see that throughout 1000 Monte-Carlo samples, while it seems that the average values of the zeroes of $P_{MPDR-SMI}(z)$ are on the unit circle, in practice, very few of them actually are. Algorithm 1 is a proposition to enable the calculation of unit-circle corrected weight. In figure \ref{fig:uc_corrected}, we notice that we indeed have corrected the weights, and they now belong to the unit-circle. Beware, in algorithm 1, to lines 5-8. They seem a bit ``heuristic'' put up this way, though they ensure that no zeroes are put in the main lobe, which would be catastrophic for the resulting weights.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\linewidth}
        \includegraphics[width=.9\linewidth]{paper/figures/uc_monte_carlo.pdf}
        \caption{Monte Carlo Sampling}
        \label{fig:uc_monte_carlo}
    \end{subfigure}\hspace{0.09\linewidth}
    \begin{subfigure}[b]{0.4\linewidth}
        \includegraphics[width=.9\linewidth]{paper/figures/uc_corrected.pdf}
        \caption{Correction of zeroes}
        \label{fig:uc_corrected}
    \end{subfigure}
    \caption{Unit Circle zeroes correction}
    \label{fig:uc_plots}
\end{figure}

\begin{algorithm}
\caption{Unit Circle MPDR weight algorithm}

\KwIn{Matrix $L$, vector $x$, vector $v_0$, scalar $N$, scalar $Q$, scalar $P$}
\KwOut{MVDR weights $w_{SMI}$ and $w_{UC}$, unit circle polynomial $P_{UC}(z)$}

% Step 1: Compute SCM
Compute SCM: $R = \frac{1}{K} \sum_{n=1}^{N} xx^H$\;

% Step 2: Compute SMI MVDR weights
Compute SMI MPDR weights: $w_{SMI} = \frac{R^{-1}a_0}{a_0^H R^{-1} a_0}$\;

% Step 3: Compute PS(z)
Compute $PS(z) = P_{SMI}(z) = \prod_{n=1}^{Q-1} (1 - \xi_n z^{-1})$ where $\xi_n = r_n e^{j\omega_n}$\;

% Step 4-8: Conditions on omega_n and xi_n
\For{$n=1$ \KwTo $Q-1$}{
    \eIf{$|\omega_n| > \frac{2\pi}{N}$}{
        $\hat{\xi}_n = e^{j\omega_n}$\;
    }{
        $\hat{\xi}_n = e^{j\text{sgn}(\omega_n) \frac{2\pi}{N}}$\;
    }
}

% Step 9: Compute PUC(z)
Compute $P_{UC}(z) = \prod_{n=1}^{Q-1} (1 - \hat{\xi}_n z^{-1}) = \sum_{n=0}^{P-1} c^*_n z^{-n}$\;

% Step 10: Define c
Define vector $c = [c_1, c_2, \ldots, c_N]$\;

% Step 11: Compute UC MVDR weight
Compute UC MVDR weight: $w_{UC} = \frac{c}{|c^H v_0|}$\;
\label{alg:uc}
\end{algorithm}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\linewidth}
        \input{paper/figures/uc_mpdr_sinr}
        \caption{SINR in function of $\Delta\theta$}
        \label{uc_mpdr_sinr}
    \end{subfigure}\hspace{0.09\linewidth}
    \begin{subfigure}[b]{0.4\linewidth}
        \input{paper/figures/uc_mpdr_awn}
        \caption{$A_{WN}$ in function of $\Delta\theta$}
        \label{fig:uc_mpdr_awn}
    \end{subfigure}
    \caption{Robustness of Unit-Circle MPDR faced to error in DOA estimation.}
    \label{fig:uc_mpdr}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\linewidth}
        \input{paper/figures/uc_mpdr_sinr_k}
        \caption{SINR in function of K}
        \label{uc_mpdr_sinr_k}
    \end{subfigure}\hspace{0.09\linewidth}
    \begin{subfigure}[b]{0.4\linewidth}
        \input{paper/figures/uc_mpdr_awn_k}
        \caption{$A_{WN}$ in function of K}
        \label{fig:uc_mpdr_awn_k}
    \end{subfigure}
    \caption{Robustness of Unit-Circle MPDR faced to number of snapshots K.}
    \label{fig:uc_mpdr_k}
\end{figure}
Figure \ref{fig:uc_mpdr} and \ref{fig:uc_mpdr_k} show that this method works well, and provides way better results than naive MPDR. Furthermore, it requires very few snapshots to perform already pretty good, and is even better than MVDR in very low snapshot conditions. 
\section{Generalized sidelobe canceler (GSC) implementation of MVDR/MPDR beamformers}
In this section, we will see that the previous Beamformers (MVDR and MPDR) can be expressed in a more intuitive way.
\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{paper/figures/gsc_canceller.png}
    \caption{GSC Beamformer}
    \label{fig:gsc_canceller}
\end{figure}
Figure \ref{fig:gsc_canceller} represents a different interpretation of the MVDR-MPDR beamformers. In the following, we will demonstrate that GSC beamformer coincides with MVDR-MPDR. In figure \ref{fig:gsc_canceller}, $\mathbf{B}$ is the so-called ``block matrix'', which is semi-unitary ($\mathbf{B}^H\mathbf{B} = \mathbf{I}_{N-1}$) such that $\mathbf{B}^H\mathbf{a}_0 = \mathbf{0}$. It's goal is to block the signal of interest, meaning $\mathbf{a}_0s(k)$.

The role of $\mathbf{w}_a$ is to retrieve the interferences $\mathbf{i}_1(k)$. The General Sidelobe Canceller (GSC) transforms the generic MVDR-MPDR beamformers into an unconstrained optimization problem. From figure \ref{fig:gsc_canceller}, we can note that 
\begin{equation}
    \mathbf{w}_{GSC} = \mathbf{w}_{CBF}-\mathbf{B}\mathbf{w}_a
\end{equation}
\begin{lemma}
    We can formulate $\mathbf{w}_{GSC}$ as an unconstrained optimization problem, while statisfying $\mathbf{w}_{GSC}^H\mathbf{a}_0=1$.
\end{lemma}
\begin{proof}
    \begin{equation*}
        \mathbf{w}_{GSC}^H\mathbf{a}_0 = \left(\mathbf{w}_{CBF}-\mathbf{B}\mathbf{w}_a\right)^H\mathbf{a}_0
    \end{equation*}
    \begin{equation*}
        \Rightarrow \mathbf{w}_{GSC}^H\mathbf{a}_0 = \underbrace{\mathbf{w}_{CBF}^H\mathbf{a}_0}_{=1}-\mathbf{w}_a^H\underbrace{\mathbf{B}^H\mathbf{a}_0}_{=0}
    \end{equation*}
    \begin{equation*}
        \Rightarrow \mathbf{w}_{GSC}^H\mathbf{a}_0 = 1
    \end{equation*}
\end{proof}
\begin{theorem}
    We can formulate GSC beamformer as $\mathbf{w}_{GSC}=\mathbf{w}_{CBF}-\mathbf{B}\mathbf{R}_z^{-1}\mathbf{r}_{dz}$ where $\mathbf{r}_{dz}=\mathbb{E}[d^*(k)\mathbf{z}(k)]$ and $\mathbf{R}_z=\mathbb{E}[\mathbf{z}(k)\mathbf{z}^H(k)]$
\end{theorem}
\begin{proof}
    Let us formulate the optimization problem:
    \begin{equation*}
        \min_{\mathbf{w}_a}\mathbb{E}\left[\left|d(k)-\mathbf{w}_a^H\mathbf{z}(k)\right|^2\right] = \min_{\mathbf{w}_a}\mathbb{E}\left[\left|d(k)\right|^2\right]-\mathbf{w}_a^H\mathbf{r}_{dz}-\mathbf{r}_{dz}^H\mathbf{w}_a+\mathbf{w}_a^H\mathbf{R}_z\mathbf{w}_a
    \end{equation*}
    We will solve this with first-order optimality.
    \begin{equation*}
        \nabla\mathbb{E}\left[\left|d(k)-\mathbf{w}_a^H\mathbf{z}(k)\right|^2\right] = 0
    \end{equation*}
    \begin{equation*}
        \Leftrightarrow -\mathbf{r}_{dz}-\mathbf{r}_{dz}^H+2\mathbf{R}_z\mathbf{w}_a = 0
    \end{equation*}
    \begin{equation*}
        \Leftrightarrow 2\mathbf{r}_{dz} = 2\mathbf{R}_z\mathbf{w}_a
    \end{equation*}
    \begin{equation*}
        \Leftrightarrow \mathbf{w}_a = \mathbf{R}_z^{-1}\mathbf{r}_{dz}
    \end{equation*}
\end{proof}
In Matlab, one can verify that $\|\mathbf{w}_{MVDR}-\mathbf{w}_{GSC}\|_ 2 = \epsilon$ with $\epsilon$ the machine precision. We conclude that the GSC and MVDR-MPDR beamformers coincides, and that figure \ref{fig:gsc_canceller} proposes a more intuitive way of formulating beamformers : being a conventional beamformer, from which we remove interferences.
\section{Partially Adaptative Beamforming}
\subsection{Reformulating MVDR Beamformer when $\mathbf{C} = \mathbf{A}_j \mathbf{R}_j \mathbf{A}_j^H + \sigma^2 \mathbf{I}_N$}

We assume that there are \( J \) interfering signals, such that \( \mathbf{C} \) can be expressed as:

\begin{equation}\label{eq:c}
\mathbf{C} = \mathbf{A}_j \mathbf{R}_j \mathbf{A}_j^H + \sigma^2 \mathbf{I}_N
\end{equation}

where \( \mathbf{A}_j = [\mathbf{a}_1, \ldots, \mathbf{a}_J] \) with \( \mathbf{a}_j \) as the steering vector of the \( j^{th} \) interference. \( \mathbf{R}_j \) represents the interfering signals covariance matrix and can be defined as \( \mathbf{R}_j = \text{diag}(P_1, \ldots, P_J) \) if the interfering signals are uncorrelated.

Utilizing the matrix inversion lemma:

\begin{equation}
(\mathbf{A} + \mathbf{BCD})^{-1} = \mathbf{A}^{-1} - \mathbf{A}^{-1} \mathbf{B} (\mathbf{C}^{-1} + \mathbf{D} \mathbf{A}^{-1} \mathbf{B})^{-1} \mathbf{D} \mathbf{A}^{-1}
\end{equation}

we can show that:

\begin{equation}
\mathbf{w}_{\text{MVDR}} = \alpha \mathbf{a}_0 - \sum_{j=1}^{J} \beta_j \mathbf{a}_j = \mathbf{w}_{\text{CBF}} - \mathbf{B} \sum_{j=1}^{J} \beta_j (\mathbf{B}^H \mathbf{a}_j)
\end{equation}

as illustrated in Figure \ref{fig:mvdr_interp}. From this, we deduce that \( \mathbf{w}_{\text{MVDR}} \) is a partially adaptive beamforming algorithm. Specifically, it belongs to a lower-than-\( N \) dimensional subspace. Indeed it can be seen from equation \ref{eq:c} that
\begin{equation}
\mathbf{C}^{-1} = \frac{1}{\sigma^2}\mathbf{I}_N-\frac{1}{\sigma^2}\mathbf{I}_N\mathbf{A}_j\left(\mathbf{R}_j^{-1}+\frac{1}{\sigma^2}\mathbf{A}_j^H\mathbf{A}_j\right)^{-1}\frac{1}{\sigma^2}\mathbf{A}_j^H
\end{equation}
Which demonstrates that $\mathbf{w}_{MVDR}$ can be written in a basis spanned by $\mathbf{a}_0$ and $\beta$. Verifying this property on Matlab, it is observed that projecting $\mathbf{w}_{MVDR}$ orthogonally onto this lower dimensional subspace, that $\Bar{\mathbf{w}}_{MVDR} = \mathbf{w}_{MVDR}$ meaning $\mathbf{w}_{MVDR}$ was already living within that subspace.

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{paper/figures/mvdr_interp.png}
    \caption{Interpretation of the MVDR beamformer}
    \label{fig:mvdr_interp}
\end{figure}
\subsection{Fixed an Adaptative Transformations}
\subsubsection{With approximately known interference locations}
In this section, we will see how one can leverage the architecture depicted in figure \ref{fig:partially_adaptative} to produce an efficient yet practical beamformer.

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{paper/figures/partially_adaptative.png}
    \caption{Partially Adaptative Beamforming}
    \label{fig:partially_adaptative}
\end{figure}

This architecture comes from the previous observation that we can work in a lower dimensional subspace, since the signal lives within it. The matrix $\mathbf{U}$ transfors $\mathbf{z}(k)$ into $\Tilde{\mathbf{z}}(k)$, living in a small subspace. The role of $\Tilde{\mathbf{w}}_a$ is to retrieve the interferences. Hence the matrix $\mathbf{U}$ must contain all interference. One condition that is necessary, but not sufficient, for this condition to hold is that $R\geq J$, with $J$ the number of interferences. More rigorously, we want $\text{rank}(\mathbf{U}) \geq J$. Optimally, we would want that the column of $\mathbf{U}$ to be equal to $\mathbf{B}^H\mathbf{a}_i$, with $\mathbf{a}_i$ the interference steering vectors.

Given we know the interference steering vectors perfectly (figure \ref{fig:partially_adaptative}), we observe that this way of constructing the MVDR beamformer is very efficient. First, it provides better results than traidtionnal MVDR, converging faster, but also, it is very robust to slight errors in the steering vectors estimations.

\begin{figure}[H]
    \centering
    \input{paper/figures/partial_mvdr_convergence}
    \caption{Convergence of the Partially Adaptative MVDR, with known steering vectors $\mathbf{a}_i$ but with limited number of snapshots.}
    \label{fig:enter-label}
\end{figure}

However, having known interference locations is quite a bold challenge.

\subsubsection{Spectral Decomposition Method}

In this part, we will investigate another method in constructing the matrix $\mathbf{U}$. To achieve this efficiently, let us leverage the spectral properties of $\mathbf{R}_z$. It can be seen from figure \ref{fig:spectrum} that interferences have a huge impact on the spectrum of $\mathbf{R}_z$. In fact, for each interference there is, one eigenvalue of $\mathbf{R}_z$ becomes abnormally large. This property can be used to count the number of interferences (hence the rank $R$ condition can be satisfied). It also underlines the necessity of having enough elements in the antenna. Indeed, if one has more than $N-1$ interferences, the number of degree of freedom is fewer than the constraints dimension, and it will thus be impossible to remove all the interferences. 

\begin{figure}[H]
    \centering
    \input{paper/figures/spectrum}
    \caption{Spectum of $\mathbf{R}_z$ with 4 interferences}
    \label{fig:spectrum}
\end{figure}

A very strong property of this is that not only can we count the number of interferences through the eigenvalues of $\mathbf{R}_z$, but we can naturally derive their locations thanks to its eigenvectors. Let us recall the eigen-decomposition of $\mathbf{R}_z$:
\begin{equation}
    \mathbf{R}_z = \sum_{n=1}^{N-1} \lambda_n\mathbf{q}_n\mathbf{q}_n^H
\end{equation}
with $\lambda_n$ the eigenvalues of $\mathbf{R}_z$ in increasing order. By taking the $R$ eigenvectors associated to the $R$ dominant eigenvalues, we can construct $\mathbf{U}$. Figure \ref{fig:eigen_decomposition} depicts the behavior of this beamformer with the number of dominant eigenvectors taken, for a system with 4 interferences. We observe that indeed, having $R\geq J$ is necessary to have a decent SNR. Also, we observe that the best case is when $R=J$, meaning if you consider the full eigen decomposition of $\mathbf{R}_z$, you will get poorer performance than only the necessary eigenvectors.

\begin{figure}[H]
    \centering
    \input{paper/figures/eigen_decomposition}
    \caption{Evolution of the SNR in function of the number of eigenvectors considered in $\mathbf{U}$.}
    \label{fig:eigen_decomposition}
\end{figure}
\subsection{Conjugate Gradient Method based beamformer}
In this section, we will see another partially adaptative beamformer, this time, we will not leverage the GSC architecture, rather, we will employ numerical linear algebra tools. Recall the expression of the MVDR beamformer:
\begin{equation}
    \mathbf{w}_{MVDR} = \frac{\mathbf{R}^{-1}\mathbf{a}_0}{\mathbf{a}_0^H\mathbf{R}^{-1}\mathbf{a}_0}
\end{equation}
in other words
\begin{equation}
    \mathbf{w}_{MVDR} \propto \mathbf{R}^{-1}\mathbf{a}_0
\end{equation}
meaning
\begin{equation}
    \mathbf{w}_{MVDR} = \mathbf{R}^{-1}\mathbf{a}_0
\end{equation}
up to a scaling factor. This can be written as a linear system
\begin{equation}\label{eq:syst}
    \mathbf{R}\mathbf{w}_{MVDR} = \mathbf{a}_0
\end{equation}
There exist plenty of method to solve such linear systems. Considering we do not have information on its size, we need to consider large-scale scenario. Krylov methods, an extension of the Power Iteration that is used to restituate the spectrum of a given matrix, are a powerful and popular set of tools. Among them, the FOM (Full Orthogonalization Method) is a popular solution. It is computationally costly but has good guarantees. However, to solve equation \ref{eq:syst}, we can leverage the inner structure of $\mathbf{R}$. Indeed, $\mathbf{R}$ is a rank-structured matrix, and more specifically, a symmetric matrix. For those structures, Lanczos based methods are very powerful. They are a subset of Krylov methods that leverage symmetry to transpose the linear system into a tridiagonal linear system through the Lanczos algorithm.

The Conjugate Gradient Method is the mathematically equivalent of FOM, but for symmetric matrices, leveraging the Lanczos tridiagonalization. The algorithm of the Conjugate Gradient Method can be seen below (Algorihtm 2).

\begin{algorithm}[H]
%\SetAlgoNlRelativeLine{1}
\SetNlSty{}{}{}
%\SetNlRelativeSize{-2}
%\SetNlRelativeSize{2}
%\SetAlgoNlRelativeSize{-2}

\caption{Conjugate Gradient Algorithm for solving \(Ax = b\)}
\KwIn{Symmetric positive-definite matrix \(A \in \mathbb{R}^{n \times n}\), vector \(b \in \mathbb{R}^{n}\)}
\KwOut{Solution vector \(x \in \mathbb{R}^{n}\)}
\BlankLine

1. Initialize \(x_0\) (an initial guess for \(x\))\;
2. Compute \(r_0 = b - Ax_0\)\;
3. Set \(p_0 = r_0\)\;
4. Set \(k = 0\)\;
\While{not converged}{
  5. Compute \(\alpha_k = \frac{r_k^T r_k}{p_k^T A p_k}\)\;
  6. Update \(x_{k+1} = x_k + \alpha_k p_k\)\;
  7. Update \(r_{k+1} = r_k - \alpha_k A p_k\)\;
  8. Check convergence. If \( ||r_{k+1}||_2 < \epsilon \), break\;
  9. Compute \(\beta_k = \frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}\)\;
  10. Update \(p_{k+1} = r_{k+1} + \beta_k p_k\)\;
  11. Set \(k = k+1\)\;
}

\end{algorithm}

Note that in this algorithm, $\alpha$ and $\beta$ respectively represent the main diagonal and the sub-upper diagonals. An important property of Kyrlov based iterative method is that the Ritz values (estimates of eigen values) converge primarly to dominant eigenvalues. This means that the first eigenvalues that will be approximates will be the interference eigenvalues. 

We can thus suppose that $J$ iteration of the Conjugate Gradient Method on equation \ref{eq:syst} should form the optimal beamformer. In figure \ref{fig:mvdr_cg}, we observe the expected result. We see very close behavior to figure \ref{fig:partially_adaptative}. Thus, the Conjugate Gradient Method based beamformer does behave very similarly to the partially adaptative based on eigen decomposition. It does seem however that the Conjugate Gradient Method is performing better when the dimension of the Kyrlov subspace is smaller than the number of interferences.

\begin{figure}[H]
    \centering
    \input{paper/figures/mvdr_cg}
    \caption{Evolution of the SINR in function of the number of iterations with the Conjugate Gradient Method}
    \label{fig:mvdr_cg}
\end{figure}

\section{Conclusion}
In this short document, we have explored various ways of retrieving signals with an element-based antenna. We most notably noticed that, while theory is very simple, in practice, we have to employ more advance technique to overcome practical challenges. We conclude that more sophisticated methods, based on eigen decomposition and conjugate gradient method, allow for efficient beamforming, while keeping many unknown on the environment.
\end{document}